论文《Towards Better Correctness and Efficiency in Code Generation》的总结：

***

## 论文大概

本论文关注于代码生成大模型的实际应用中，生成代码虽正确但运行效率较低的问题。作者提出一种面向效率的强化学习框架，结合新型性能奖励，改善模型生成代码的正确性和效率。提出了两阶段训练方法：首先用离线优化保障正确性，然后通过在线强化学习提升效率，最终得到兼顾正确率和运行效率的高质量代码生成模型。[1]

***

## 论文提出的问题

- 代码大语言模型生成的代码在功能正确性方面表现优异，但运行效率普遍较低，实际部署时比人工代码慢 3~13 倍，严重影响代码落地价值。
- 离线微调方法受限于静态数据，难以超越数据集中的高效实现，无法发现新算法。
- 在线强化学习虽然能动态探索，但奖励信号噪声大，容易引入不稳定性和系统性错误。
- 代码优化涉及平衡正确率与效率，需要设计新的训练和奖励机制。[1]

***

## 论文解决办法

- 提出了面向效率的两阶段优化方法：
    - **阶段一**：用正确性为主的数据对模型离线微调，确保高基础正确率。
    - **阶段二**：用误差不敏感的在线强化学习算法（如 RLOO），通过高对比效率奖励提升运行效率，同时保持已获得的高正确率。
- 奖励函数结合代码运行效率（CPU 指令数，经过对数处理降噪）、问题难度和错误类型，系统性评估生成代码的性能。
- 强调用高复杂度、高辨识度的数据输入区分不同代码效率，保证训练过程中效率优化的效果可靠。[1]

***

## 实验

- 采用 Qwen-2.5-Coder-Instruct-7B 模型，在 EvalPerf 和 Mercury 两大基准集上比较 SFT, DPO, GRPO, RLOO、多阶段方法等多种训练方式。
- 两阶段方法在代码正确率和效率方面均优于单阶段训练，在 7B 模型上正确率提升 10.18%，效率提升 7.75%，性能接近更大模型。[1]
- 验证了不同初始化（如高准确率模型、不同效率奖励比例）、不同 RL 算法（RLOO vs GRPO）、高复杂度输入等对最终效率和正确性的影响。[1]
- 案例分析显示模型能够从暴力解法推理到高效算法实现，但也存在通过缓存等手段“奖励作弊”问题，揭示评测机制的待改进空间。[1]

***

## 总结

本文系统分析了代码生成任务中正确性与效率的平衡，发现离线方法适合提升正确性，在线误差不敏感方法（RLOO）适合提升效率。提出的两阶段策略大幅提升了代码生成的正确性和效率，为代码 LLM 的应用落地提供了新思路。实验证实方法优越，案例揭示未来优化方向。[1]

***

## QA

### 所以具体的奖励机制是怎么设计的，离线训练的数据是怎么样的，强化学习阶段的数据是怎么样的？


**1. 奖励机制设计**

- 采用 CPU 指令数（而不是 wall-clock 时间）作为代码效率评估指标，减少非确定性误差。[1]
- 对指令数取对数：$$ C = -\log(N) $$，其中 N 为指令数，C 越大代表执行越快。[1]
- 多方案对比而非单一参考：对同一问题题解进行高复杂度输入测试，生成一组性能分数 G，对比同一问题所有正确代码。[1]
- 奖励函数 F(C, G)：最慢的正确代码得分为 1.0，最快的最高为 2.0，其他依线性插值决定。具体公式如下：

  $$
  F(C, G) = S_{min} + \frac{(C - min(G)) \cdot (S_{max} - S_{min})}{max(G) - min(G)}
  $$
  其中 $$S_{min} = 1.0$$, $$S_{max} = min(2.0, S_{min} + max(G) - min(G))$$。

- 测试通过：奖励为 F(C,G)；测试错误：奖励 0；缺失实体/函数：-0.5；格式错误：-1.5。[1]



**2. 离线训练的数据**

- 基于人类高效代码（如 LeetCode，Evalperf，Mercury，EffiBench），对大量实际算法题做采样。[1]
- SFT 数据集：优选每题最快且正确的代码，实现初始模型的高正确率。[1]
- DPO 数据集：正确性对比对（正确 vs 错误）以及效率对比对（快的 vs 慢的正确代码），90%权重用于正确性、10%用于效率对比，来保证离线微调结果更偏向正确率。[1]
- 具体训练数据规模：EffiBench（790题），Effi-Learner（2978题合成），主用 LeetCode 和 EvalPlus 问题。[1]



**3. 强化学习阶段数据**

- RL 阶段用所有训练集样本（如 Effi-Learner、EffiBench），但数据输入采用高复杂度、强区分性的样例，从而区分算法效率。[1]
- 用 RLOO 算法（error-insensitive RL）优化，强化奖励机制，对输入进行复杂变异并注重效率信号。[1]
- 初始化模型为已用 DPO 微调后高准确率版本，再通过 RL 专注于效率提升，输入分布更复杂。
- RL 训练超参数：例如，30 episodes、学习率 1e-6、batch size 64、生成代码最大长度 12k 字符。[1]




[论文链接](https://arxiv.org/pdf/2508.20124.pdf)
