论文《Towards Better Correctness and Efficiency in Code Generation》的总结：

***

## 论文大概

本论文关注于代码生成大模型的实际应用中，生成代码虽正确但运行效率较低的问题。作者提出一种面向效率的强化学习框架，结合新型性能奖励，改善模型生成代码的正确性和效率。提出了两阶段训练方法：首先用离线优化保障正确性，然后通过在线强化学习提升效率，最终得到兼顾正确率和运行效率的高质量代码生成模型。[1]

***

## 论文提出的问题

- 代码大语言模型生成的代码在功能正确性方面表现优异，但运行效率普遍较低，实际部署时比人工代码慢 3~13 倍，严重影响代码落地价值。
- 离线微调方法受限于静态数据，难以超越数据集中的高效实现，无法发现新算法。
- 在线强化学习虽然能动态探索，但奖励信号噪声大，容易引入不稳定性和系统性错误。
- 代码优化涉及平衡正确率与效率，需要设计新的训练和奖励机制。[1]

***

## 论文解决办法

- 提出了面向效率的两阶段优化方法：
    - **阶段一**：用正确性为主的数据对模型离线微调，确保高基础正确率。
    - **阶段二**：用误差不敏感的在线强化学习算法（如 RLOO），通过高对比效率奖励提升运行效率，同时保持已获得的高正确率。
- 奖励函数结合代码运行效率（CPU 指令数，经过对数处理降噪）、问题难度和错误类型，系统性评估生成代码的性能。
- 强调用高复杂度、高辨识度的数据输入区分不同代码效率，保证训练过程中效率优化的效果可靠。[1]

***

## 实验

- 采用 Qwen-2.5-Coder-Instruct-7B 模型，在 EvalPerf 和 Mercury 两大基准集上比较 SFT, DPO, GRPO, RLOO、多阶段方法等多种训练方式。
- 两阶段方法在代码正确率和效率方面均优于单阶段训练，在 7B 模型上正确率提升 10.18%，效率提升 7.75%，性能接近更大模型。[1]
- 验证了不同初始化（如高准确率模型、不同效率奖励比例）、不同 RL 算法（RLOO vs GRPO）、高复杂度输入等对最终效率和正确性的影响。[1]
- 案例分析显示模型能够从暴力解法推理到高效算法实现，但也存在通过缓存等手段“奖励作弊”问题，揭示评测机制的待改进空间。[1]

***

## 总结

本文系统分析了代码生成任务中正确性与效率的平衡，发现离线方法适合提升正确性，在线误差不敏感方法（RLOO）适合提升效率。提出的两阶段策略大幅提升了代码生成的正确性和效率，为代码 LLM 的应用落地提供了新思路。实验证实方法优越，案例揭示未来优化方向。[1]

***

[1](https://arxiv.org/pdf/2508.20124.pdf)
